# -*- coding: utf-8 -*-
"""spam dectection dp project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Flo9GUTQQt-BNBo9HH2IYOSRSeTLcwim

# spam dectection
"""

import pandas as pd
data=pd.read_csv('/content/spam (1).csv',encoding='latin1')
data

data.info()

del data['Unnamed: 2']
del data['Unnamed: 3']
del data['Unnamed: 4']

data=data[['v1','v2']]
data.columns=['label','message']
data.head()

data.groupby('label').size() #groupby used to group unique value

import seaborn as sns
import matplotlib.pyplot as plt
data['label'].value_counts().plot(kind='bar')

import re         # RE Library for Regular Expression
import nltk       # NLTK Library for Natural Language Processing
nltk.download('stopwords')
from nltk.corpus import stopwords   # Stopwords for removing stopwords in the Text
from nltk.stem.porter import PorterStemmer # PorterStemmer for Stemming the Words
ps=PorterStemmer()
corpus=[]
for i in range(0,len(data)):
  review=re.sub('[^a-zA-Z]',' ',data['message'][i])
  review=review.lower()
  review=review.split()
  review=[ps.stem(word) for  word in review if not word in stopwords.words('english')]
  review=' '.join(review)
  corpus.append(review)

# Printing the first 5 values in the corpus list
corpus[1:6]

from sklearn.feature_extraction.text import CountVectorizer

cv=CountVectorizer(max_features=4000)
x=cv.fit_transform(corpus).toarray()
y=pd.get_dummies(data['label'])
y=y.iloc[:,1].values

from sklearn.model_selection import train_test_split

xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.20,random_state=42)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB

m1=RandomForestClassifier()
m1.fit(xtrain,ytrain)

m2=DecisionTreeClassifier()
m2.fit(xtrain,ytrain)

m3=MultinomialNB()
m3.fit(xtrain,ytrain)

pred1=m1.predict(xtest)
pred2=m2.predict(xtest)
pred3=m3.predict(xtest)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score


print('random forest classifier')
print('confusion matrix:')
print(confusion_matrix(ytest,pred1))
print('accuracy:', accuracy_score(ytest,pred1))
print('--------------------------------------')

print('Decision Tree Classifier')
print('confusion matrix:')
print(confusion_matrix(ytest,pred2))
print('accuracy:', accuracy_score(ytest,pred2))
print('--------------------------------------')

print('Multinomial Naive bayes')
print('confusion matrix:')
print(confusion_matrix(ytest,pred3))
print('accuracy:', accuracy_score(ytest,pred3))
print('--------------------------------------')

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(ytest,pred3)

import seaborn as sns
sns.heatmap(cm,annot=True)

from sklearn.metrics import classification_report

report1=classification_report(ytest,pred1)
print('classification Report for rfc \n',report1)

report2=classification_report(ytest,pred2)
print('classification Report for rfc \n',report2)

report3=classification_report(ytest,pred3)
print('classification Report for rfc \n',report3)

import pickle

filename="rfc.pkl"
pickle.dump(m1,open(filename,'wb'))

filename="dtc.pkl"
pickle.dump(m2,open(filename,'wb'))

filename="mnb.pkl"
pickle.dump(m3,open(filename,'wb'))
print('saved all models')

"""## custom message"""

import pandas as pd
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data = pd.read_csv('/content/5000 YT comments.csv', encoding='latin1')
data

data = data[['Spam','Comment']]
data.columns = ['label','message']
data

# data['label'] = data['label'].map({'ham':0, 'spam':1})
# data

data['label'].value_counts()

# Preprocessing function
ps = PorterStemmer()
def preprocess(text):
    review = re.sub('[^a-zA-Z]', ' ', text)  # keep only letters
    review = review.lower().split()
    review = [ps.stem(word) for word in review if word not in stopwords.words('english')]
    return ' '.join(review)

data['message'] = data['message'].apply(preprocess)
data

xtrain, xtest, ytrain, ytest = train_test_split(data['message'], data['label'], test_size=0.2, random_state=42)

# Build pipeline with TF-IDF + Naive Bayes
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=4000)),   # feature extraction
    ('model', MultinomialNB())                      # classifier
])

# Train
pipeline.fit(xtrain, ytrain)

# Evaluate
ypred = pipeline.predict(xtest)

print("Accuracy:", accuracy_score(ytest, ypred))
print("Confusion Matrix:\n", confusion_matrix(ytest, ypred))
print("Classification Report:\n", classification_report(ytest, ypred))

# Predict on custom messages
spam = ['she is a girl ,her name is sona',
        'sun rise is beautiful',
        'Congratulations! ðŸŽ‰ Youâ€™ve won a FREE iPhone 15! Click here to claim your prize now: http://fake-prize-link.com  Hurry, offer expires in 24 hours! ðŸš€ ',
        'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&Cs apply 08452810075over18s']

print("Predictions:", pipeline.predict(spam))   # 0 = ham, 1 = spam

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Example: df has ['label', 'message']
# label: 0 = ham, 1 = spam
df = pd.read_csv("/content/5000 YT comments.csv", encoding='latin1')
df = df[['Spam','Comment']]
df.columns = ['label','message']

X = df['message']
y = df['label']

# Convert text â†’ TF-IDF vectors (same as your pipeline)
vectorizer = TfidfVectorizer(max_features=4000)
X_tfidf = vectorizer.fit_transform(X).toarray()

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y, test_size=0.2, random_state=42
)

# Build Deep Learning model (Dense Neural Network)
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.3))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))  # binary output

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train
history = model.fit(X_train, y_train,
                    epochs=5,
                    batch_size=64,
                    validation_data=(X_test, y_test))

# Evaluate
loss, acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {acc:.4f}")

spam = [
    'she is a girl ,her name is sona',
    'sun rise is beautiful',
    'Congratulations! ðŸŽ‰ Youâ€™ve won a FREE iPhone 15! Click here to claim your prize now: http://fake-prize-link.com  Hurry, offer expires in 24 hours! ðŸš€ ',
    'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&Cs apply 08452810075over18s'
]
X_custom = vectorizer.transform(spam).toarray()

# Predict probabilities
y_pred = model.predict(X_custom)
y_pred_labels = (y_pred > 0.5).astype(int)

for msg, pred in zip(spam, y_pred_labels):
    label = "SPAM" if pred == 1 else "HAM"
    print(f"{msg[:50]}... => {label}")

